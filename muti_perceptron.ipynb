{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check if all the prerequisites are there.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = []\n",
    "outputs = []\n",
    "\n",
    "d = {}\n",
    "d['Iris-setosa\\n']     = [1., 0., 0.]\n",
    "d['Iris-versicolor\\n'] = [0., 1., 0.]\n",
    "d['Iris-virginica\\n']  = [0., 0., 1.]\n",
    "\n",
    "with open('iris.data') as f:\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        items=line.split(',')\n",
    "        if len(items) == 5:\n",
    "            inp =  [float(x) for x in items[0:4] ]\n",
    "            inputs.append(inp)\n",
    "            out = d[items[4]]\n",
    "            outputs.append(out)\n",
    "            \n",
    "print( len(inputs), 'input patterns', len(outputs), 'output patterns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=random.sample(range(0,len(inputs)), 100) # generate 100 random ids\n",
    "#training set\n",
    "train_in = []\n",
    "train_out=[]\n",
    "for id in ids:\n",
    "    train_in.append(inputs[id])\n",
    "    train_out.append(outputs[id])\n",
    "train_inputs  = np.array(train_in)\n",
    "train_outputs = np.array(train_out)\n",
    "\n",
    "#valid set\n",
    "val_input =[]\n",
    "val_output=[]\n",
    "validation_ids = list(set(range(0,len(inputs))) - set(ids))\n",
    "for val_id in validation_ids:\n",
    "    val_input.append(inputs[val_id])\n",
    "    val_output.append(outputs[val_id])\n",
    "val_inputs  = np.array(val_input)\n",
    "val_outputs = np.array(val_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(train_accuracy_list,valid_accuracy_list,epoch): # plot performance over the training epochs\n",
    "    epochs = range(len(train_accuracy_list))\n",
    "    nb_epochs    = len(epochs)\n",
    "    plt.axis((0,nb_epochs,0,1.2))\n",
    "    plt.plot(epochs, train_accuracy_list, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, valid_accuracy_list, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setosa_sign = [1., 0., 0.]\n",
    "versicolor_sign = [0., 1., 0.]\n",
    "virginica_sign = [0., 0., 1.]\n",
    "verify_type_temp = setosa_sign #choose one of flower type to train the perceptron\n",
    "\n",
    "verify_type = 0\n",
    "for s in range(3): \n",
    "    if verify_type_temp[s] == 1: \n",
    "        verify_type = s\n",
    "        break\n",
    "train_accuracy_list = []\n",
    "valid_accuracy_list = []\n",
    "lr = 0.2\n",
    "epoch = 300\n",
    "# lr = 0.1       #if do not want to introduce learning rate and epoch,\n",
    "# epoch = 1000     # we can set the lr and epoch equal 1\n",
    "log_time = 10 \n",
    "log_step = epoch/log_time\n",
    "init_weight = np.random.uniform(-10,10,5)\n",
    "weight = init_weight\n",
    "for i in range(epoch):\n",
    "  trainset_collect_classify_time = 0\n",
    "  for j in range(len(train_inputs)):\n",
    "      data_point = np.append(train_inputs[j],[1])\n",
    "      #calclulate the classification\n",
    "      if(train_outputs[j][verify_type] == 0):\n",
    "          if((weight.dot(data_point)) > 0):\n",
    "              weight = weight - lr * data_point\n",
    "          else:\n",
    "            trainset_collect_classify_time +=1\n",
    "      elif(train_outputs[j][verify_type] == 1):\n",
    "          if(weight.dot(data_point) < 0):\n",
    "              weight = weight + lr * data_point\n",
    "          else:\n",
    "            trainset_collect_classify_time +=1\n",
    "  train_accuracy = trainset_collect_classify_time/100\n",
    "  validset_collect_classify_time = 0\n",
    "  for j in range(len(val_input)):\n",
    "      data_point = np.append(val_input[j],[1])\n",
    "      if(val_output[j][verify_type] == 1):\n",
    "        if(data_point.dot(weight) > 0):\n",
    "          validset_collect_classify_time +=1\n",
    "      elif(val_output[j][verify_type] == 0):\n",
    "        if(data_point.dot(weight) < 0):\n",
    "          validset_collect_classify_time +=1\n",
    "  val_accuracy = validset_collect_classify_time/50\n",
    "  if i % log_step == 0 or i == (epoch-1):\n",
    "      print(\"epoch:[{}/{}]  train_accuracy:{}   valid accuracy:{}\".format(i+1,epoch,train_accuracy,val_accuracy))\n",
    "      train_accuracy_list.append(train_accuracy)\n",
    "      valid_accuracy_list.append(val_accuracy)\n",
    "show_results(train_accuracy_list,valid_accuracy_list,epoch)\n",
    "print(weight)"
   ]
  }
 ]
}